# -*- coding: utf-8 -*-
"""MidTerm_Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MlwRZsOsstkC-zPmuooTboGK3aQ2ycAb
"""

# Commented out IPython magic to ensure Python compatibility.
# Install YOLOv5 dependencies
!git clone https://github.com/ultralytics/yolov5  # Clone YOLOv5 repo
# %cd yolov5
!pip install -r requirements.txt  # Install dependencies

from google.colab import drive
drive.mount('/content/drive')

import torch

# Load YOLOv5 model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load the small YOLOv5 model

import cv2
from google.colab.patches import cv2_imshow

# Path to your uploaded video file
video_path = '/content/drive/MyDrive/Data Eki/MVI_6943 (online-video-cutter.com) (2).mp4'  # Update with your video file path

cap = cv2.VideoCapture(video_path)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)  # Perform inference on the frame
    frame_with_boxes = results.render()[0]  # Draw bounding boxes on the frame

    # Display the output frame using cv2_imshow
    cv2_imshow(frame_with_boxes)

    # To slow down the display, you can use a delay
    cv2.waitKey(1)  # Optional: adjust the delay as needed

cap.release()

# Step 2: Import necessary libraries
import torch
import cv2
from IPython.display import display, Video

# Step 3: Set device for optimization
device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available

# Step 4: Load the pre-trained YOLOv5 model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)

# Step 5: Load video
video_path = '/content/drive/MyDrive/Data Eki/MVI_6943.MP4'  # Change this to your video path
cap = cv2.VideoCapture(video_path)

# Get video properties
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Define the codec and create VideoWriter object to save processed video
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4
output_video_path = '/content/output_video.mp4'  # Output path for the processed video
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

# Step 6: Process video frames
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Step 7: Perform inference on the frame
    results = model(frame)  # Pass original frame to the model without resizing

    # Step 8: Process results and display bounding boxes
    annotated_frame = results.render()[0]  # Render the results on the frame

    # Write the annotated frame to the output video
    out.write(annotated_frame)

# Release resources
cap.release()
out.release()
cv2.destroyAllWindows()

!git init
!git remote add origin https://github.com/KironoDwis/Midterm_Task.git
!git pull origin main
!git add .
!git commit -m "Update notebook with YOLO processing"
!git push origin main

